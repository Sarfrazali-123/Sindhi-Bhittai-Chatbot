from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
import os

# Set your OpenAI API key (replace with your actual key)
os.environ["OPENAI_API_KEY"] = "api-key-pasted"

# Import necessary libraries for your RAG model
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.docstore.document import Document
from langchain_community.document_loaders import TextLoader

app = Flask(__name__)
CORS(app)  # Enable CORS for all routes

# Initialize the RAG model components
def initialize_rag_model():
    print("Initializing RAG model...")
    
    # Read the file
    with open('.\\ganj_data.txt', 'r', encoding='utf-8') as file:
        text = file.read()

    ### FIX: Convert text into a list of `Document` objects ###
    documents = [Document(page_content=text)]  # Fixed here

    # Split into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    docs = text_splitter.split_documents(documents)  # Now `docs` is a list of Document objects

    # Create vector store
    embedding_function = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents=docs, embedding=embedding_function)
    retriever = vectorstore.as_retriever()

    # Define prompt template
    
    prompt = PromptTemplate.from_template(
        """
ØªÙˆÙ‡Ø§Ù† Ú¾Úª Ù…Ø¯Ø¯Ú¯Ø§Ø± Ø¢Ú¾ÙŠÙˆ Ø¬ÙŠÚªÙˆ Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠ Ø¨Ø§Ø¨Øª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÚÙŠÚ» Û¾ Ù…Ø¯Ø¯ ÚªÙ†Ø¯Ùˆ Ø¢Ú¾ÙŠ. Ú¾ÙŠÙº ÚÙ†Ù„ Ø­ÙˆØ§Ù„ÙŠ (Context) Ù…Ø§Ù† ØµØ­ÙŠØ­ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø§ØµÙ„ ÚªØ±ÙŠÙˆ Û½ Ø³ÙˆØ§Ù„ Ø¬Ùˆ Ø¬ÙˆØ§Ø¨ ÚÙŠÙˆ.

Ù…Ø«Ø§Ù„:
Q: Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠ Ø¬ÙŠ ÙˆÙ„Ø§Ø¯Øª ÚªÚÙ‡Ù† Ù¿ÙŠØŸ
Context: Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠØ¡Ù Ø¬Ù† Ø¬Ùˆ Ø¬Ù†Ù…ØŒ Ø³Ù† 1689Ø¹ Û¾ Ø­ÙŠØ¯Ø±Ø¢Ø¨Ø§Ø¯ Ø¶Ù„Ø¹ÙŠ Ø¬ÙŠ Ù‡Ø§Ú»ÙˆÚªÙŠ Ù½Ù†ÚŠÙŠ Ø¢Ø¯Ù… ØªØ¹Ù„Ù‚ÙŠ Ø¬ÙŠ Ù‡Úª Ú³ÙˆÙº Ø³ÙØ¦ÙŠ ÚªÙ†ÚŒØ±ØŒ Ù‡Ø§Ù„Ø§ Ø­ÙˆÙŠÙ„ÙŠ Û¾ Ù¿ÙŠÙˆ.
A: Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠØ¡Ù Ø¬ÙŠ ÙˆÙ„Ø§Ø¯Øª 1689Ø¹ Û¾ Ø­ÙŠØ¯Ø±Ø¢Ø¨Ø§Ø¯ Ø¶Ù„Ø¹ÙŠ Ø¬ÙŠ Ú³ÙˆÙº Ø³ÙØ¦ÙŠ ÚªÙ†ÚŒØ±ØŒ Ù‡Ø§Ù„Ø§ Ø­ÙˆÙŠÙ„ÙŠ Û¾ Ù¿ÙŠ.

Q: Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠØ¡Ù Ø¬Ùˆ ÙˆÙØ§Øª ÚªÚÙ‡Ù† Ù¿ÙŠØŸ
Context: Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠØ¡Ù Ø¬Ùˆ ÙˆÙØ§Øª Ø³Ù† 1752Ø¹ Û¾ 63 Ø³Ø§Ù„Ù† Ø¬ÙŠ Ø¹Ù…Ø± Û¾ Ù¿ÙŠ.
A: Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠØ¡Ù Ø¬Ùˆ ÙˆÙØ§Øª 22 ÚŠØ³Ù…Ø¨Ø± 1752Ø¹ Û¾ 63 Ø³Ø§Ù„Ù† Ø¬ÙŠ Ø¹Ù…Ø± Û¾ Ù¿ÙŠ.

Ø³ÙˆØ§Ù„: {question}
Ø­ÙˆØ§Ù„Ùˆ (Context): {context}

Ø¬ÙˆØ§Ø¨:
"""
    )

    # Set up language model
    llm = ChatOpenAI(
        model_name="gpt-4",
        temperature=0.3
    )

    # Format docs function
    def format_docs(docs_list):
        return "\n\n".join(doc.page_content for doc in docs_list)  # This was failing earlier because docs was a string

    # Create the RAG chain
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    print("RAG model initialized successfully!")
    return rag_chain

# Initialize the RAG model
try:
    rag_chain = initialize_rag_model()
    print("RAG model loaded successfully")
except Exception as e:
    print(f"Error initializing RAG model: {str(e)}")
    rag_chain = None

# Routes
@app.route('/')
def home():
    return render_template('index.html')

@app.route('/static/<path:path>')
def serve_static(path):
    return send_from_directory('static', path)

@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.json
    user_message = data.get('message', '')

    if not user_message:
        return jsonify({"response": "ÚªØ¬Ú¾ Ù¾Ú‡Ú» Ø¬ÙŠ Ù„Ø§Ø¡Ù Ù…Ú¾Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ ÚªØ¬Ú¾ Ù„Ú©Ùˆ!"})

    # Check for basic predefined responses first
    predefined_responses = {
        "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÚªÙ…": "ÙˆØ¹Ù„ÙŠÚªÙ… Ø§Ù„Ø³Ù„Ø§Ù…! Ø§ÙˆÙ‡Ø§Ù† ÚªÙŠØ¦Ù† Ø¢Ù‡ÙŠÙˆØŸ ğŸ˜Š",
        "ØªÙˆÙ‡Ø§Ù† Ø¬Ùˆ Ù†Ø§Ù„Ùˆ Ú‡Ø§ Ø¢Ù‡ÙŠØŸ": "Ù…Ø§Ù† Ù‡Úª Ø³Ù†ÚŒÙŠ Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¢Ù‡ÙŠØ§Ù†.",
        "Ø§ÙˆÙ‡Ø§Ù† Ú‡Ø§ ÚªØ±ÙŠ Ø³Ú¯Ù‡Ùˆ Ù¿Ø§ØŸ": "Ù…Ø§Ù† Ø´Ø§Ú¾ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ø·ÙŠÙ Ú€Ù½Ø§Ø¦ÙŠ Ø¨Ø§Ø¨Øª Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÚØ¦ÙŠ Ø³Ú¯Ù‡Ø§Ù† Ù¿Ùˆ!",
        "Ø®Ø¯Ø§Ø­Ø§ÙØ¸": "Ø§Ù„ÙˆØ¯Ø§Ø¹! Ø§Ù„Ù„Ù‡ ÙˆØ§Ù‡ÙŠ ğŸ’™"
    }

    if user_message in predefined_responses:
        return jsonify({"response": predefined_responses[user_message]})

    # Use the RAG model for more complex queries
    if rag_chain:
        try:
            response = rag_chain.invoke(user_message)

            print("response", response)
            return jsonify({"response": response})
        
        except Exception as e:
            print(f"Error invoking RAG chain: {str(e)}")
            return jsonify({"response": f"Ù…Ø¹Ø§Ù ÚªØ¬ÙˆØŒ Ù‡Úª Ø®Ø±Ø§Ø¨ÙŠ Ø¢Ø¦ÙŠ Ø¢Ù‡ÙŠ: {str(e)}"})
    else:
        return jsonify({"response": "Ù…ÙˆÚŠÙ„ Ù„ÙˆÚŠ Ù†Ù‡ Ù¿ÙŠ Ø³Ú¯Ù‡ÙŠÙˆ"})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
